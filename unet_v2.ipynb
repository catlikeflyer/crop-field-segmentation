{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0, 255, 255],\n",
       " 1: [255, 255, 0],\n",
       " 2: [255, 0, 255],\n",
       " 3: [0, 255, 0],\n",
       " 4: [0, 0, 255],\n",
       " 5: [255, 255, 255],\n",
       " 6: [0, 0, 0]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"./deepglobe-land-cover-classification-dataset/train\"\n",
    "COLOR_CODES = \"./deepglobe-land-cover-classification-dataset/class_dict.csv\"\n",
    "\n",
    "# Define the U-Net model\n",
    "def get_unet_model(num_classes):\n",
    "    model = smp.Unet(\n",
    "        encoder_name=\"resnet34\",        # You can use different encoders like resnet18, resnet50, etc.\n",
    "        encoder_weights=\"imagenet\",     # You can also initialize the encoder with weights pretrained on ImageNet\n",
    "        in_channels=3,                  # Number of input channels (RGB)\n",
    "        classes=num_classes,            # Number of output classes\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # Normalize with ImageNet mean and std\n",
    "])\n",
    "\n",
    "df = pd.read_csv(COLOR_CODES)\n",
    "label_map = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    label_map[index] = [row[\"r\"],row[\"g\"],row[\"b\"]]\n",
    "    \n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmentation_Dataset(Dataset):\n",
    "    def __init__(self, image_dir, label_map, transform):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.label_map = label_map\n",
    "        self.images_name = sorted([filename for filename in os.listdir(self.image_dir) if filename.endswith('_sat.jpg') and not filename.startswith(\"._\")])\n",
    "        self.targets_name = sorted([filename for filename in os.listdir(self.image_dir) if filename.endswith('_mask.png') and not filename.startswith(\"._\")])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images_name)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.images_name[idx])\n",
    "        mask_path = os.path.join(self.image_dir, self.targets_name[idx])\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert RGB\n",
    "        \n",
    "        mask = cv2.imread(mask_path)\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "        mask = self.colormap_to_labelmap(mask)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "            \n",
    "        return {\"image\": image, \"mask\": mask}\n",
    "    \n",
    "    def colormap_to_labelmap(self, mask):\n",
    "        label_image = np.zeros_like(mask[:,:,0], dtype=np.uint8)\n",
    "\n",
    "        for label, color in self.label_map.items():\n",
    "            color_array = np.array(color)\n",
    "            mask_condition = np.all(mask == color_array, axis=-1)\n",
    "            label_image[mask_condition] = label\n",
    "\n",
    "        return label_image.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "big_dataset = Segmentation_Dataset(DATA_DIR, label_map, transform)\n",
    "[train_dataset, val_dataset, test_dataset] = torch.utils.data.random_split(big_dataset,[0.75,0.15,0.1], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "valid_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "train_batch = next(iter(train_dataloader))\n",
    "\n",
    "print(train_batch['image'].shape)\n",
    "print(train_batch['mask'].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x3162864a0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.transpose(train_batch['mask'][0], (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(image))\n\u001b[0;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, mask)\n\u001b[1;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/segmentation_models_pytorch/base/model.py:27\u001b[0m, in \u001b[0;36mSegmentationModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     25\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m     30\u001b[0m     decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\u001b[38;5;241m*\u001b[39mfeatures)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/segmentation_models_pytorch/base/model.py:14\u001b[0m, in \u001b[0;36mSegmentationModel.check_input_shape\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_input_shape\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 14\u001b[0m     h, w \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m     15\u001b[0m     output_stride \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39moutput_stride\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m h \u001b[38;5;241m%\u001b[39m output_stride \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m w \u001b[38;5;241m%\u001b[39m output_stride \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "num_classes = 7  # Adjust based on the number of classes in DeepGlobe dataset\n",
    "model = get_unet_model(num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for image, mask in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        print(type(image))\n",
    "        outputs = model(image)\n",
    "        loss = criterion(outputs, mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for image, mask in valid_dataloader:\n",
    "            outputs = model(image)\n",
    "            val_loss += criterion(outputs, mask).item()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss/len(valid_dataloader):.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'unet_deepglobe.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
